#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v2.0 - Enhanced Analysis Pipeline
Pipeline de an√°lise aprimorado com continuidade garantida e zero simula√ß√£o
"""

import time
import logging
import json
from datetime import datetime
from typing import Dict, List, Any, Optional, Callable
from services.ai_manager import ai_manager
from services.ultra_robust_search_manager import ultra_robust_search_manager
from services.mental_drivers_architect import mental_drivers_architect
from services.visual_proofs_generator import visual_proofs_generator
from services.anti_objection_system import anti_objection_system
from services.enhanced_pre_pitch_architect import enhanced_pre_pitch_architect
from services.future_prediction_engine import future_prediction_engine
from services.auto_save_manager import auto_save_manager, salvar_etapa, salvar_erro
from services.analysis_quality_controller import analysis_quality_controller

logger = logging.getLogger(__name__)

class EnhancedAnalysisPipeline:
    """Pipeline de an√°lise aprimorado com continuidade garantida"""
    
    def __init__(self):
        """Inicializa pipeline aprimorado"""
        self.components = [
            ('pesquisa_web_ultra_robusta', self._execute_ultra_robust_search),
            ('analise_ia_avancada', self._execute_advanced_ai_analysis),
            ('avatar_ultra_detalhado', self._extract_avatar_from_analysis),
            ('drivers_mentais_customizados', self._generate_mental_drivers),
            ('provas_visuais_instantaneas', self._generate_visual_proofs),
            ('sistema_anti_objecao', self._generate_anti_objection),
            ('pre_pitch_invisivel', self._generate_pre_pitch),
            ('predicoes_futuro_completas', self._generate_future_predictions),
            ('insights_exclusivos_finais', self._generate_final_insights)
        ]
        
        self.quality_filters = {
            'min_content_length': 3000,
            'min_sources': 3,
            'min_quality_score': 60.0,
            'simulation_tolerance': 0  # Zero toler√¢ncia
        }
        
        logger.info("Enhanced Analysis Pipeline inicializado")
    
    def execute_complete_analysis(
        self, 
        data: Dict[str, Any],
        session_id: str = None,
        progress_callback: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """Executa an√°lise completa com continuidade garantida"""
        
        start_time = time.time()
        session_id = session_id or auto_save_manager.iniciar_sessao()
        
        logger.info(f"üöÄ Iniciando pipeline aprimorado para {data.get('segmento')}")
        
        # Salva dados de entrada
        salvar_etapa("pipeline_iniciado", {
            "input_data": data,
            "session_id": session_id,
            "components_count": len(self.components)
        }, categoria="analise_completa")
        
        # Executa componentes com continuidade garantida
        results = {}
        successful_components = []
        failed_components = []
        
        for i, (component_name, component_func) in enumerate(self.components):
            if progress_callback:
                progress_callback(i + 1, f"Executando {component_name}...")
            
            try:
                logger.info(f"üîÑ Executando componente: {component_name}")
                
                # Executa componente com dados acumulados
                component_data = {**data, **results}
                result = component_func(component_data, session_id)
                
                if result and self._validate_component_result(component_name, result):
                    results[component_name] = result
                    successful_components.append(component_name)
                    
                    # Salva resultado imediatamente
                    salvar_etapa(f"componente_{component_name}", result, categoria="analise_completa")
                    
                    logger.info(f"‚úÖ {component_name}: Sucesso")
                else:
                    failed_components.append(component_name)
                    logger.warning(f"‚ö†Ô∏è {component_name}: Resultado inv√°lido ou vazio")
                    
                    # Salva falha mas continua
                    salvar_erro(f"componente_{component_name}", 
                              Exception("Resultado inv√°lido"), 
                              contexto={"component": component_name})
                
            except Exception as e:
                failed_components.append(component_name)
                logger.error(f"‚ùå {component_name}: {str(e)}")
                
                # Salva erro mas CONTINUA pipeline
                salvar_erro(f"componente_{component_name}", e, 
                          contexto={"component": component_name, "data": component_data})
        
        # Consolida an√°lise final (SEMPRE gera algo)
        final_analysis = self._consolidate_final_analysis(
            data, results, successful_components, failed_components, session_id
        )
        
        # Filtra dados brutos do relat√≥rio final
        clean_analysis = self._filter_raw_data_from_report(final_analysis)
        
        # Adiciona metadados
        processing_time = time.time() - start_time
        clean_analysis['metadata'] = {
            'processing_time_seconds': processing_time,
            'session_id': session_id,
            'successful_components': successful_components,
            'failed_components': failed_components,
            'success_rate': len(successful_components) / len(self.components) * 100,
            'generated_at': datetime.now().isoformat(),
            'pipeline_version': '2.0_enhanced',
            'simulation_free': True,
            'raw_data_filtered': True
        }
        
        # Salva an√°lise final
        salvar_etapa("analise_final_limpa", clean_analysis, categoria="analise_completa")
        
        logger.info(f"‚úÖ Pipeline conclu√≠do: {len(successful_components)}/{len(self.components)} sucessos")
        
        return clean_analysis
    
    def _execute_ultra_robust_search(self, data: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Executa pesquisa ultra-robusta aprimorada"""
        
        try:
            # Gera queries expandidas e inteligentes
            queries = self._generate_enhanced_queries(data)
            
            # Executa busca em m√∫ltiplas camadas
            search_results = []
            
            for query in queries[:5]:  # Top 5 queries
                try:
                    results = ultra_robust_search_manager.execute_comprehensive_search(
                        query, data, max_results=20, require_high_quality=True
                    )
                    
                    if results.get('meets_quality_requirements'):
                        search_results.append(results)
                        
                except Exception as e:
                    logger.warning(f"Query '{query}' falhou: {e}")
                    continue
            
            if not search_results:
                raise Exception("Nenhuma busca retornou dados de qualidade suficiente")
            
            # Consolida resultados
            consolidated = self._consolidate_search_results(search_results)
            
            return {
                'queries_executadas': queries,
                'resultados_consolidados': consolidated,
                'total_fontes': consolidated.get('total_sources', 0),
                'qualidade_media': consolidated.get('avg_quality', 0),
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Pesquisa ultra-robusta falhou: {e}")
            raise Exception(f"PESQUISA FALHOU: {str(e)}")
    
    def _execute_advanced_ai_analysis(self, data: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Executa an√°lise avan√ßada com IA"""
        
        try:
            search_data = data.get('pesquisa_web_ultra_robusta', {})
            
            # Prepara contexto de pesquisa limpo
            search_context = self._prepare_clean_search_context(search_data)
            
            # Prompt aprimorado
            prompt = self._build_enhanced_analysis_prompt(data, search_context)
            
            # Executa com IA
            response = ai_manager.generate_analysis(prompt, max_tokens=8192)
            
            if not response:
                raise Exception("IA n√£o respondeu")
            
            # Processa e valida resposta
            analysis = self._process_and_validate_ai_response(response, data)
            
            return analysis
            
        except Exception as e:
            logger.error(f"An√°lise IA falhou: {e}")
            raise Exception(f"AN√ÅLISE IA FALHOU: {str(e)}")
    
    def _extract_avatar_from_analysis(self, data: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Extrai avatar da an√°lise IA"""
        
        try:
            ai_analysis = data.get('analise_ia_avancada', {})
            avatar = ai_analysis.get('avatar_ultra_detalhado', {})
            
            if not avatar or not avatar.get('perfil_demografico'):
                raise Exception("Avatar insuficiente na an√°lise IA")
            
            # Valida qualidade do avatar
            if not self._validate_avatar_quality(avatar):
                raise Exception("Avatar n√£o atende crit√©rios de qualidade")
            
            return avatar
            
        except Exception as e:
            logger.error(f"Extra√ß√£o de avatar falhou: {e}")
            raise Exception(f"AVATAR FALHOU: {str(e)}")
    
    def _generate_mental_drivers(self, data: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Gera drivers mentais sem fallbacks"""
        
        try:
            avatar_data = data.get('avatar_ultra_detalhado', {})
            
            if not avatar_data:
                raise Exception("Avatar necess√°rio para drivers mentais")
            
            drivers = mental_drivers_architect.generate_complete_drivers_system(avatar_data, data)
            
            if not drivers or drivers.get('fallback_mode'):
                raise Exception("Drivers mentais retornaram fallback - rejeitado")
            
            return drivers
            
        except Exception as e:
            logger.error(f"Drivers mentais falharam: {e}")
            raise Exception(f"DRIVERS MENTAIS FALHARAM: {str(e)}")
    
    def _generate_visual_proofs(self, data: Dict[str, Any], session_id: str) -> List[Dict[str, Any]]:
        """Gera provas visuais sem fallbacks"""
        
        try:
            avatar_data = data.get('avatar_ultra_detalhado', {})
            
            # Extrai conceitos para prova
            concepts = self._extract_proof_concepts(avatar_data, data)
            
            if not concepts:
                raise Exception("Nenhum conceito v√°lido para provas visuais")
            
            proofs = visual_proofs_generator.generate_complete_proofs_system(concepts, avatar_data, data)
            
            if not proofs or any(p.get('fallback_mode') for p in proofs):
                raise Exception("Provas visuais retornaram fallback - rejeitado")
            
            return proofs
            
        except Exception as e:
            logger.error(f"Provas visuais falharam: {e}")
            raise Exception(f"PROVAS VISUAIS FALHARAM: {str(e)}")
    
    def _generate_anti_objection(self, data: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Gera sistema anti-obje√ß√£o sem fallbacks"""
        
        try:
            avatar_data = data.get('avatar_ultra_detalhado', {})
            objections = avatar_data.get('objecoes_reais', [])
            
            if not objections:
                # Extrai obje√ß√µes das dores
                dores = avatar_data.get('dores_viscerais', [])
                objections = [f"Obje√ß√£o relacionada a: {dor}" for dor in dores[:5]]
            
            anti_obj = anti_objection_system.generate_complete_anti_objection_system(
                objections, avatar_data, data
            )
            
            if not anti_obj or anti_obj.get('fallback_mode'):
                raise Exception("Sistema anti-obje√ß√£o retornou fallback - rejeitado")
            
            return anti_obj
            
        except Exception as e:
            logger.error(f"Sistema anti-obje√ß√£o falhou: {e}")
            raise Exception(f"ANTI-OBJE√á√ÉO FALHOU: {str(e)}")
    
    def _generate_pre_pitch(self, data: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Gera pr√©-pitch sem fallbacks"""
        
        try:
            drivers_data = data.get('drivers_mentais_customizados', {})
            avatar_data = data.get('avatar_ultra_detalhado', {})
            
            drivers_list = drivers_data.get('drivers_customizados', [])
            
            if not drivers_list:
                raise Exception("Drivers necess√°rios para pr√©-pitch")
            
            pre_pitch = enhanced_pre_pitch_architect.generate_enhanced_pre_pitch_system(
                drivers_data, avatar_data, data
            )
            
            if not pre_pitch or pre_pitch.get('status') == 'EMERGENCY_MODE':
                raise Exception("Pr√©-pitch retornou modo de emerg√™ncia - rejeitado")
            
            return pre_pitch
            
        except Exception as e:
            logger.error(f"Pr√©-pitch falhou: {e}")
            raise Exception(f"PR√â-PITCH FALHOU: {str(e)}")
    
    def _generate_future_predictions(self, data: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Gera predi√ß√µes do futuro"""
        
        try:
            segmento = data.get('segmento', 'neg√≥cios')
            
            predictions = future_prediction_engine.predict_market_future(
                segmento, data, horizon_months=36
            )
            
            if not predictions:
                raise Exception("Predi√ß√µes do futuro falharam")
            
            return predictions
            
        except Exception as e:
            logger.error(f"Predi√ß√µes futuras falharam: {e}")
            raise Exception(f"PREDI√á√ïES FUTURAS FALHARAM: {str(e)}")
    
    def _generate_final_insights(self, data: Dict[str, Any], session_id: str) -> List[str]:
        """Gera insights finais consolidados"""
        
        try:
            # Coleta insights de todos os componentes
            all_insights = []
            
            # Insights da an√°lise IA
            ai_analysis = data.get('analise_ia_avancada', {})
            ai_insights = ai_analysis.get('insights_exclusivos', [])
            if ai_insights:
                all_insights.extend(ai_insights[:10])
            
            # Insights da pesquisa
            search_data = data.get('pesquisa_web_ultra_robusta', {})
            search_insights = self._extract_insights_from_search(search_data)
            all_insights.extend(search_insights)
            
            # Insights dos componentes avan√ßados
            component_insights = self._extract_component_insights(data)
            all_insights.extend(component_insights)
            
            # Remove duplicatas e filtra qualidade
            unique_insights = []
            seen_insights = set()
            
            for insight in all_insights:
                if (insight and 
                    len(insight) > 50 and 
                    insight not in seen_insights and
                    not self._is_simulated_insight(insight)):
                    unique_insights.append(insight)
                    seen_insights.add(insight)
            
            return unique_insights[:25]  # Top 25 insights √∫nicos
            
        except Exception as e:
            logger.error(f"Gera√ß√£o de insights finais falhou: {e}")
            return []  # Retorna lista vazia em vez de falhar
    
    def _consolidate_final_analysis(
        self, 
        original_data: Dict[str, Any],
        results: Dict[str, Any],
        successful: List[str],
        failed: List[str],
        session_id: str
    ) -> Dict[str, Any]:
        """Consolida an√°lise final SEMPRE (mesmo com falhas)"""
        
        # Estrutura base sempre presente
        final_analysis = {
            'projeto_dados': original_data,
            'pipeline_status': {
                'componentes_executados': successful,
                'componentes_falharam': failed,
                'taxa_sucesso': len(successful) / len(self.components) * 100,
                'session_id': session_id
            }
        }
        
        # Adiciona componentes bem-sucedidos
        for component_name in successful:
            if component_name in results:
                final_analysis[component_name] = results[component_name]
        
        # Se tem pelo menos pesquisa e an√°lise IA, considera v√°lido
        if ('pesquisa_web_ultra_robusta' in results and 
            'analise_ia_avancada' in results):
            final_analysis['status'] = 'COMPLETO'
        elif len(successful) >= 3:
            final_analysis['status'] = 'PARCIAL_VALIDO'
        else:
            final_analysis['status'] = 'MINIMO_PRESERVADO'
        
        return final_analysis
    
    def _filter_raw_data_from_report(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Remove dados brutos do relat√≥rio final"""
        
        clean_analysis = {}
        
        for key, value in analysis.items():
            if key in ['projeto_dados', 'pipeline_status', 'metadata']:
                # Mant√©m dados estruturais
                clean_analysis[key] = value
            elif isinstance(value, dict):
                # Filtra dicion√°rios recursivamente
                clean_analysis[key] = self._filter_dict_raw_data(value)
            elif isinstance(value, list):
                # Filtra listas
                clean_analysis[key] = self._filter_list_raw_data(value)
            else:
                # Mant√©m outros tipos
                clean_analysis[key] = value
        
        return clean_analysis
    
    def _filter_dict_raw_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Filtra dados brutos de dicion√°rios"""
        
        # Campos que cont√™m dados brutos a serem removidos
        raw_data_fields = [
            'extracted_content', 'raw_content', 'page_content', 'html_content',
            'search_results', 'urls_found', 'links_extracted', 'raw_response',
            'full_content', 'content_preview', 'detailed_results', 'sources_raw',
            'extraction_details', 'raw_data', 'content_raw', 'html_raw'
        ]
        
        filtered = {}
        
        for key, value in data.items():
            if key.lower() in [field.lower() for field in raw_data_fields]:
                # Remove dados brutos mas mant√©m estat√≠sticas
                if isinstance(value, list):
                    filtered[f"{key}_count"] = len(value)
                elif isinstance(value, str):
                    filtered[f"{key}_length"] = len(value)
                # N√£o inclui o conte√∫do bruto
            elif isinstance(value, dict):
                filtered[key] = self._filter_dict_raw_data(value)
            elif isinstance(value, list):
                filtered[key] = self._filter_list_raw_data(value)
            else:
                filtered[key] = value
        
        return filtered
    
    def _filter_list_raw_data(self, data: List[Any]) -> List[Any]:
        """Filtra dados brutos de listas"""
        
        filtered = []
        
        for item in data:
            if isinstance(item, dict):
                # Remove campos de conte√∫do bruto
                clean_item = {}
                for key, value in item.items():
                    if key.lower() not in ['content', 'raw_content', 'html', 'full_text']:
                        if isinstance(value, dict):
                            clean_item[key] = self._filter_dict_raw_data(value)
                        elif isinstance(value, list):
                            clean_item[key] = self._filter_list_raw_data(value)
                        else:
                            clean_item[key] = value
                    else:
                        # Mant√©m apenas estat√≠sticas do conte√∫do
                        if isinstance(value, str):
                            clean_item[f"{key}_length"] = len(value)
                
                filtered.append(clean_item)
            else:
                filtered.append(item)
        
        return filtered
    
    def _generate_enhanced_queries(self, data: Dict[str, Any]) -> List[str]:
        """Gera queries aprimoradas para pesquisa"""
        
        segmento = data.get('segmento', '')
        produto = data.get('produto', '')
        publico = data.get('publico', '')
        
        queries = []
        
        # Queries principais aprimoradas
        if produto:
            queries.extend([
                f"an√°lise mercado {segmento} {produto} Brasil 2024 dados estat√≠sticas crescimento",
                f"competi√ß√£o {segmento} {produto} principais players market share",
                f"tend√™ncias {segmento} {produto} inova√ß√£o tecnologia futuro",
                f"consumidor {segmento} {produto} comportamento compra decis√£o",
                f"pre√ßos {segmento} {produto} ticket m√©dio benchmarks mercado"
            ])
        else:
            queries.extend([
                f"mercado {segmento} Brasil 2024 tamanho crescimento oportunidades",
                f"an√°lise competitiva {segmento} principais empresas l√≠deres",
                f"tend√™ncias {segmento} inova√ß√£o disrup√ß√£o tecnol√≥gica",
                f"investimentos {segmento} venture capital funding startups",
                f"regulamenta√ß√£o {segmento} mudan√ßas legais impacto neg√≥cios"
            ])
        
        # Queries espec√≠ficas por p√∫blico
        if publico:
            queries.extend([
                f"perfil {publico} {segmento} dados demogr√°ficos IBGE",
                f"comportamento {publico} {segmento} pesquisa consumo",
                f"jornada compra {publico} {segmento} processo decis√£o"
            ])
        
        # Queries de intelig√™ncia avan√ßada
        queries.extend([
            f"cases sucesso {segmento} empresas brasileiras unic√≥rnios",
            f"fus√µes aquisi√ß√µes {segmento} M&A Brasil consolida√ß√£o",
            f"IPO {segmento} bolsa valores B3 mercado capitais",
            f"pesquisas mercado {segmento} institutos consultoria dados",
            f"relat√≥rios setoriais {segmento} McKinsey BCG Deloitte"
        ])
        
        return queries[:12]  # Top 12 queries
    
    def _consolidate_search_results(self, search_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Consolida resultados de m√∫ltiplas buscas"""
        
        total_sources = 0
        total_content_length = 0
        quality_scores = []
        all_sources = []
        
        for result in search_results:
            stats = result.get('statistics', {})
            total_sources += stats.get('successful_extractions', 0)
            total_content_length += stats.get('total_content_length', 0)
            
            if stats.get('avg_quality_score'):
                quality_scores.append(stats['avg_quality_score'])
            
            # Coleta fontes (sem conte√∫do bruto)
            sources = result.get('search_results', [])
            for source in sources:
                all_sources.append({
                    'title': source.get('title', ''),
                    'url': source.get('url', ''),
                    'source': source.get('source', ''),
                    'quality_score': source.get('filtro', {}).get('prioridade', 0)
                })
        
        return {
            'total_sources': total_sources,
            'total_content_length': total_content_length,
            'avg_quality': sum(quality_scores) / len(quality_scores) if quality_scores else 0,
            'unique_domains': len(set(s['url'].split('/')[2] for s in all_sources if s.get('url'))),
            'sources_summary': all_sources[:20]  # Top 20 fontes sem conte√∫do
        }
    
    def _prepare_clean_search_context(self, search_data: Dict[str, Any]) -> str:
        """Prepara contexto de pesquisa limpo (sem dados brutos)"""
        
        consolidated = search_data.get('resultados_consolidados', {})
        
        context = f"""PESQUISA ULTRA-ROBUSTA EXECUTADA COM SUCESSO:

ESTAT√çSTICAS DA PESQUISA:
- Total de fontes analisadas: {consolidated.get('total_sources', 0)}
- Dom√≠nios √∫nicos consultados: {consolidated.get('unique_domains', 0)}
- Qualidade m√©dia das fontes: {consolidated.get('avg_quality', 0):.1f}%
- Total de conte√∫do analisado: {consolidated.get('total_content_length', 0):,} caracteres

FONTES PRINCIPAIS CONSULTADAS:
"""
        
        sources = consolidated.get('sources_summary', [])
        for i, source in enumerate(sources[:10], 1):
            context += f"{i}. {source.get('title', 'Sem t√≠tulo')}\n"
            context += f"   URL: {source.get('url', '')}\n"
            context += f"   Fonte: {source.get('source', 'unknown')}\n"
            context += f"   Qualidade: {source.get('quality_score', 0):.1f}\n\n"
        
        context += "\nGARANTIA: Todos os dados baseados em pesquisa real, sem simula√ß√µes."
        
        return context
    
    def _build_enhanced_analysis_prompt(self, data: Dict[str, Any], search_context: str) -> str:
        """Constr√≥i prompt aprimorado para an√°lise"""
        
        return f"""# AN√ÅLISE ULTRA-DETALHADA APRIMORADA - ARQV30 v2.0

Voc√™ √© o DIRETOR SUPREMO DE AN√ÅLISE DE MERCADO com 30+ anos de experi√™ncia.

## DADOS DO PROJETO:
- Segmento: {data.get('segmento', 'N√£o informado')}
- Produto: {data.get('produto', 'N√£o informado')}
- P√∫blico: {data.get('publico', 'N√£o informado')}
- Pre√ßo: R$ {data.get('preco', 'N√£o informado')}
- Objetivo Receita: R$ {data.get('objetivo_receita', 'N√£o informado')}

{search_context}

## INSTRU√á√ïES CR√çTICAS:
1. Use APENAS dados REAIS da pesquisa
2. NUNCA use "N/A", "Customizado para", "Baseado em"
3. Seja ULTRA-ESPEC√çFICO com dados concretos
4. Se n√£o houver dados suficientes, omita a se√ß√£o
5. Foque em INSIGHTS ACION√ÅVEIS √∫nicos

## FORMATO OBRIGAT√ìRIO:
```json
{{
  "avatar_ultra_detalhado": {{
    "nome_ficticio": "Nome espec√≠fico baseado no segmento",
    "perfil_demografico": {{
      "idade": "Faixa espec√≠fica com dados reais",
      "genero": "Distribui√ß√£o real com percentuais",
      "renda": "Faixa real baseada em pesquisas",
      "escolaridade": "N√≠vel real predominante",
      "localizacao": "Regi√µes reais de concentra√ß√£o",
      "profissao": "Ocupa√ß√µes reais mais comuns"
    }},
    "perfil_psicografico": {{
      "personalidade": "Tra√ßos reais dominantes",
      "valores": "Valores reais principais",
      "interesses": "Interesses reais espec√≠ficos",
      "comportamento_compra": "Processo real documentado",
      "influenciadores": "Quem realmente influencia",
      "medos_profundos": "Medos reais documentados",
      "aspiracoes_secretas": "Aspira√ß√µes reais identificadas"
    }},
    "dores_viscerais": [
      "Lista de 10-15 dores REAIS espec√≠ficas do segmento"
    ],
    "desejos_secretos": [
      "Lista de 10-15 desejos REAIS profundos"
    ],
    "objecoes_reais": [
      "Lista de 8-12 obje√ß√µes REAIS espec√≠ficas"
    ],
    "linguagem_interna": {{
      "frases_dor": ["Frases reais que usam"],
      "frases_desejo": ["Frases reais de desejo"],
      "vocabulario_especifico": ["Palavras espec√≠ficas do nicho"],
      "tom_comunicacao": "Tom real de comunica√ß√£o"
    }}
  }},
  
  "posicionamento_estrategico": {{
    "posicionamento_mercado": "Posicionamento √∫nico baseado em an√°lise real",
    "proposta_valor_unica": "Proposta irresist√≠vel baseada em gaps reais",
    "diferenciais_competitivos": [
      "Lista de diferenciais √∫nicos e defens√°veis"
    ],
    "mensagem_central": "Mensagem principal que resume tudo",
    "estrategia_oceano_azul": "Como criar mercado sem concorr√™ncia",
    "ancoragem_preco": "Como ancorar pre√ßo na mente do cliente"
  }},
  
  "analise_concorrencia_avancada": [
    {{
      "nome": "Nome real do concorrente principal",
      "posicionamento": "Como se posicionam realmente",
      "forcas": ["For√ßas reais espec√≠ficas"],
      "fraquezas": ["Fraquezas reais explor√°veis"],
      "vulnerabilidades": ["Pontos fracos espec√≠ficos"],
      "estrategia_marketing": "Estrat√©gia real observada",
      "share_estimado": "Participa√ß√£o estimada real"
    }}
  ],
  
  "estrategia_marketing_completa": {{
    "palavras_chave_primarias": [
      "15-20 palavras principais com dados reais"
    ],
    "palavras_chave_secundarias": [
      "25-35 palavras secund√°rias identificadas"
    ],
    "long_tail_keywords": [
      "30-50 palavras de cauda longa espec√≠ficas"
    ],
    "estrategia_conteudo": "Como usar palavras-chave estrategicamente",
    "canais_aquisicao": [
      "Canais reais mais eficazes para o segmento"
    ],
    "funil_conversao": {{
      "topo": "Estrat√©gias reais para topo do funil",
      "meio": "Estrat√©gias reais para meio do funil", 
      "fundo": "Estrat√©gias reais para fundo do funil"
    }}
  }},
  
  "metricas_kpis_avancados": {{
    "kpis_principais": [
      {{
        "metrica": "Nome da m√©trica real",
        "objetivo": "Valor objetivo baseado em dados",
        "benchmark": "Benchmark real do mercado"
      }}
    ],
    "projecoes_financeiras": {{
      "cenario_conservador": {{
        "receita_mensal": "Valor real baseado em dados",
        "clientes_mes": "N√∫mero real estimado",
        "ticket_medio": "Ticket real do mercado"
      }},
      "cenario_realista": {{
        "receita_mensal": "Valor real baseado em dados",
        "clientes_mes": "N√∫mero real estimado", 
        "ticket_medio": "Ticket real do mercado"
      }},
      "cenario_otimista": {{
        "receita_mensal": "Valor real baseado em dados",
        "clientes_mes": "N√∫mero real estimado",
        "ticket_medio": "Ticket real do mercado"
      }}
    }}
  }},
  
  "insights_exclusivos": [
    "Lista de 20-30 insights ULTRA-ESPEC√çFICOS e ACION√ÅVEIS baseados exclusivamente na pesquisa real"
  ]
}}
```

CR√çTICO: Gere APENAS o JSON v√°lido. Use exclusivamente dados da pesquisa real."""
    
    def _process_and_validate_ai_response(self, response: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """Processa e valida resposta da IA rigorosamente"""
        
        try:
            # Extrai JSON
            clean_text = response.strip()
            
            if "```json" in clean_text:
                start = clean_text.find("```json") + 7
                end = clean_text.rfind("```")
                clean_text = clean_text[start:end].strip()
            
            # Parse JSON
            analysis = json.loads(clean_text)
            
            # Valida√ß√£o rigorosa
            validation = analysis_quality_controller.validate_complete_analysis(analysis)
            
            if not validation['valid']:
                raise Exception(f"An√°lise IA inv√°lida: {validation['errors']}")
            
            # Verifica simula√ß√µes
            if self._contains_simulation_markers(analysis):
                raise Exception("An√°lise cont√©m marcadores de simula√ß√£o")
            
            return analysis
            
        except json.JSONDecodeError as e:
            raise Exception(f"JSON inv√°lido da IA: {str(e)}")
    
    def _contains_simulation_markers(self, analysis: Dict[str, Any]) -> bool:
        """Verifica se cont√©m marcadores de simula√ß√£o"""
        
        analysis_str = json.dumps(analysis, ensure_ascii=False).lower()
        
        forbidden_phrases = [
            'customizado para', 'baseado em', 'espec√≠fico para', 'n/a',
            'n√£o informado', 'exemplo de', 'simulado', 'gen√©rico'
        ]
        
        return any(phrase in analysis_str for phrase in forbidden_phrases)
    
    def _validate_component_result(self, component_name: str, result: Any) -> bool:
        """Valida resultado de componente"""
        
        if not result:
            return False
        
        # Rejeita fallbacks
        if isinstance(result, dict) and result.get('fallback_mode'):
            return False
        
        # Valida√ß√µes espec√≠ficas por componente
        if component_name == 'avatar_ultra_detalhado':
            return self._validate_avatar_quality(result)
        elif component_name == 'drivers_mentais_customizados':
            return self._validate_drivers_quality(result)
        elif component_name == 'insights_exclusivos_finais':
            return len(result) >= 5 if isinstance(result, list) else False
        
        return True
    
    def _validate_avatar_quality(self, avatar: Dict[str, Any]) -> bool:
        """Valida qualidade do avatar"""
        
        required_fields = ['perfil_demografico', 'dores_viscerais', 'desejos_secretos']
        
        for field in required_fields:
            if not avatar.get(field):
                return False
        
        # Verifica se dores e desejos s√£o substanciais
        dores = avatar.get('dores_viscerais', [])
        desejos = avatar.get('desejos_secretos', [])
        
        if len(dores) < 5 or len(desejos) < 5:
            return False
        
        # Verifica se n√£o s√£o gen√©ricas
        for dor in dores[:3]:
            if self._is_simulated_insight(dor):
                return False
        
        return True
    
    def _validate_drivers_quality(self, drivers: Dict[str, Any]) -> bool:
        """Valida qualidade dos drivers"""
        
        drivers_list = drivers.get('drivers_customizados', [])
        
        if len(drivers_list) < 3:
            return False
        
        for driver in drivers_list:
            if not driver.get('nome') or not driver.get('roteiro_ativacao'):
                return False
            
            historia = driver.get('roteiro_ativacao', {}).get('historia_analogia', '')
            if len(historia) < 100:
                return False
        
        return True
    
    def _is_simulated_insight(self, insight: str) -> bool:
        """Verifica se insight √© simulado"""
        
        if not insight or len(insight) < 30:
            return True
        
        simulation_indicators = [
            'customizado para', 'baseado em', 'espec√≠fico para',
            'exemplo de', 'simulado', 'gen√©rico', 'n/a'
        ]
        
        insight_lower = insight.lower()
        return any(indicator in insight_lower for indicator in simulation_indicators)
    
    def _extract_insights_from_search(self, search_data: Dict[str, Any]) -> List[str]:
        """Extrai insights da pesquisa (sem dados brutos)"""
        
        insights = []
        consolidated = search_data.get('resultados_consolidados', {})
        
        # Insights baseados em estat√≠sticas
        total_sources = consolidated.get('total_sources', 0)
        if total_sources > 0:
            insights.append(f"An√°lise baseada em {total_sources} fontes reais de alta qualidade")
        
        avg_quality = consolidated.get('avg_quality', 0)
        if avg_quality > 70:
            insights.append(f"Qualidade m√©dia das fontes: {avg_quality:.1f}% - dados confi√°veis")
        
        unique_domains = consolidated.get('unique_domains', 0)
        if unique_domains > 5:
            insights.append(f"Diversidade de fontes: {unique_domains} dom√≠nios √∫nicos consultados")
        
        return insights
    
    def _extract_component_insights(self, data: Dict[str, Any]) -> List[str]:
        """Extrai insights dos componentes avan√ßados"""
        
        insights = []
        
        # Insights dos drivers mentais
        drivers = data.get('drivers_mentais_customizados', {})
        if drivers and not drivers.get('fallback_mode'):
            drivers_count = len(drivers.get('drivers_customizados', []))
            insights.append(f"Sistema de {drivers_count} drivers mentais customizados implementado")
        
        # Insights das provas visuais
        proofs = data.get('provas_visuais_instantaneas', [])
        if proofs and not any(p.get('fallback_mode') for p in proofs):
            insights.append(f"Sistema de {len(proofs)} provas visuais instant√¢neas desenvolvido")
        
        # Insights do sistema anti-obje√ß√£o
        anti_obj = data.get('sistema_anti_objecao', {})
        if anti_obj and not anti_obj.get('fallback_mode'):
            insights.append("Sistema anti-obje√ß√£o completo com scripts personalizados")
        
        return insights

# Inst√¢ncia global
enhanced_analysis_pipeline = EnhancedAnalysisPipeline()